* Chapter 11 File-System Implementation
** File-System Structure
  - basic file system :: issues generic commands to appropriate device drivers
       | Layered Ô¨Åle system       |
       | ------------------------ |
       | application programs     |
       | file-organization module |
       | logical file system      |
       | basic file system        |
       | I/O control              |
       | devices                  |
       v v
       
  - file-organization module :: knows about files and their logical blocks
       as well as physical blocks. This module can translate logical
       block addresses to phyical blocked addresses. Also provides
       free-space manager
       
  - logical file system :: manages metadata
    - all file system structure except the actual data (i.e contents of files)
    - provies directory structure to provide file organization module
    - maintains file structure via file-control block (inode in UNIX)
    - responsible for protection and security
      
  - file-control block (FCB) :: contains information about the file, including
       ownership, permissions, and location of file contents.
       | Typical FCB                                      |
       | ------------------------------------------------ |
       | file permissions                                 |
       | file dates (create, access, write)               |
       | file owner, group, ACL                           |
       | file size                                        |
       | file data blocks or pointers to file data blocks |
       
** File-System Implmentation
   Information in file systems
   - boot control block :: info needed by system to boot an OS from that volume
        
   - volume control block :: contains partition details such as number of blocks
        in partition, size of blocks, free block-count, free-block
        pointers, free-FCB count, and FCB pointers.  In UFS called a
        superblock.  In NTFS caled the master file table
        
        
   The in-memory information is loaded at mount time, updated during
   file-system operations, and discared at dismount. Several types of
   structires are involved:
   - mount table :: in memory infomation about each monuted volume
                    
   - system-wide open-file table :: contains a copy of FCB of each open file and
        other information
        
   - per-process open-file table :: contains pointers to appropriate entry in
        the system-wide open-file table
        
   - buffers to hold file-system blocks when being read from disk or written
     to disk
     
   File Creation - application program calls the logical file system
   where the LFS allocates a new FCB, reads the appropriate directory
   into memory, updates it with new file name and FCB, and writes it
   back to disk.
   
   Note: UNIX treats a directory exactly the same as a file (includes
   a "type" field indicating it is a directory
   
   Once file is created, it can be used for I/O. open() call passed a
   file name to the logical file system. First, search done on
   system-wide open-file table to see if the file is already in use by
   another process. If not, directory structure is searched for the
   given file name. Once file is found, the FCB is copied into the
   system-wide open-file table in memory. This table not only stores
   the FCB but also tracks the number of processes that have file
   open.
   
   - file descriptor (UNIX), file handle (Windows) :: name given to the entry
        in the open-file table
        
   When file is closed, per-process table entry is removed, and
   system-wide entry's open count is decremented. When all users that
   have opened the file close, any updated metadata is copied back to
   the disk-based directory structure, and system- wide open file
   table entry is removed.
   
** Virtual File Systems
   Why? To allow and operating system to allow multiple types of file
   systems to be integrated into a directory structure
   
** Directory Implementation
   Directory-allocation and directory-management algorithms
   
   Linear List
   - Linear list of file names with pointers to the data blocks 
     
   Hash Table
   - Hash table takes a value computed from the file name and return a pointer
     to the file name in the linear list therefore greatly decreasing
     the directory search time.
     
** Allocation Methods
   Direct-access nature of disks allows flexibility in the
   implementation of files.  Main isue is how to allocate space to
   these files so that disk space is utilized effectively and files
   can be accessed quickly. Three magor methods of allocating disk
   space:
   
*** Contiguous Allocation
    Requires each file occupy a set of contiguous blocks on the
    disk. Disk addresses define a linear ordering on the disk.
    
    Has problems in finding space for a new file. Either first-fit or
    best-fit algorithms are used but can results in external
    fragmentation.
    
    To make efficient use of disk space, modified
    contiguous-allocation schemes exists. If an amount allocated to a
    file proves to not be large enough, another chunk of contiguous
    space, known as *extent* is allocated.
    
*** Linked Allocation
    Solves all problems of contiguous allocation:
    - Each file is a linked list of disk blocks meaning the disk blocks can be
      scattered anywhere on disk
    - Directory contrains pointer to first and last block of the file
    - Each block contains a pointer to the next block
      - block is 512 bytes in size, 4 bytes of which are pointers to the 
        next block thus user sees blocks of 508 bytes
        
    _File creation_ Create a new entry in the directory where each
    directory entry has a pointer to the first disk block of the
    file. This pointer is initialized to nil to signifiy an empty
    file. Size field also set to 0. A write will cause the free-space
    management system to find a free block and this new block is
    written to and is linked to the end of the file
    
    _Disadvantage_
    - Can be used effectively only for sequential-access files. Inefficient
      direct access support if underlying structure is linkedin list
    - 4 bytes out of 512 are for pointers which means 0.78 percent of
      disk is being used for pointers (Solution: disadvantage of
      wasted pointer space is to collect blocks into multiples called
      *clusters* and allocated clusters at a time.)
    - Too much reliability on pointers being valid. If a bug were to corrupt
      pointer, wrong data blocks would be retrieved

    - file-allocation table :: An important variation on linked allocation
      + Section of disk at beginning of each volume is set aside to contain 
        the table
      + Table has one entry for each disk block and is indexed by block number
      + Directory entry contains the block number of the first block of the file
      + Table entry indexed by block number contains block number of next block 
        in the file.
      + This chain is continued until the last block is reached which has special
        end-of-file value as table entry
      + Unused block has table value of 0. Block allocation is simple
        matter of finding first 0-valued table entry and replacing
        previous end-of-file value with address of new block. The 0 is
        then replaced with end-of-file value
      + benefit of this approach is that the FAT table can be cached
        in memory, greatly improving random access speeds.

        
*** Indexed Allocation
    Indexed allocation solves external-fragmentation and
    size-declaration problem of contiguous allocation. All pointers of
    a file are brough together in what is known as the *index block*
    (array of disk-block addresses)

   - Each file has own index block
   - Directory contains the address of the index block
   - When file is created, all pointers in index block are set to nil
   - When ith block is first written, a block is obtained from the
     free-space manager, and its address is put in the ith index-block
     entry

   Pros: Supports direct access without suffering from external
   fragmentation Cons: Suffers from wasted space. Pointer overhead is
   generally greater than pointer overhead of linked allocation

   _How large should index block be?_
   - Linked scheme :: link index blocks
   - Multilevel index :: Variant of linked scheme where first-leve
        index block to point to a set of second-level index blocks,
        which in turn point to the file blocks. I.e with 4096-byte
        blocks, we could store 1024 four-byte pointers in an index
        block. Two levels of indexes allow 1048576 (1024^2) data
        blocks and a file size up to 4 GB.
        
        
   Indexed-allocation scheme suffer form some of the same performance problems
   as does linked allocation. The index block can be cached in memory,
   but the data blocks may be spread all over a volume

** Performance concerns
    For any type of access (random or sequential), contiguous allocation
    requires only once access to get a disk block. We can easily keep the initial
    memory address of the file in memory and can calculate the other blocks via
    offset of the initial

    For linked allocation, address of the next block can be kept in memory 
    and read directly. Although this works fine for sequential access, for direct
    access, an access to the ith block may require i disk reads.

** Free-Space Management
   - free-space list :: To keep track of free disk space. Records all
        free disk blocks. Despite its name, may not be implemented as a list

   - bit vector :: free-space list implemented as bit map or bit vector. 
                   Each bloack is represented by 1 bit. If block is free,
                   the bit is 1, if block is allocated, bit is 0.
                   Cons - inefficient unless entire vector is kept
                   in memory, but then it takes up more memory as size of
                   disk increases

   - linked list :: link together all free disk blocks. FAT method incorporates
                    free-block accounting into the allocation data structure

** Efficieny and Performance
   - UNIX inodes are preallocated on a volume for efficiency even though 
     this means an "empty" disk has a percentage of its space list to inodes.
     However, by preallocation inodes and spreading them across the volume,
     improve fs performance.
     Neat - UNIX tries to keep a file's data block near that file's inode block
     to reduce disk seek time.

* Chapter 7 Deadlocks
** Deadlock characterization
   Deadlock can only arise iff conditions hold simultaneously
   - Mutual Exclusion :: At least one resource held in a nonsharable mode
   - Hold and wait :: Process must hold at least one resource and wait to acquire
                      additional resource held by other processes
   - No preemption :: resource can only be released voluntarily by process holding it
   - Circular wait :: A set {P0,P1,...,Pn} waiting processes must exist such that
                      P0 waits on P1, P1 waits on P2, ... , Pn waits on P0.

   With resource allocation graphs, if graphs contains no cycles, then no process in
   the system is deadlocked. Deadlock can exists iff there is a cycle (but not necessarily
   occured)

** Methods for handling deadlocks
   - Deadlock prevention :: provides methods to ensure at least one of the necessary
        conditions does not hold
   - Deadlock avoidance :: OS given in advance additional information concerning which
        resource a process will request and use
   - Deadlock ignorance :: Since deadlocks occur infrequently, allow then to take up
        resources until system reboot

*** Deadlock prevention
**** Mutual Exclusion
     Have only sharable resources
**** Hold and wait
     Must have it whenever process requests a resource, it doesn't hold any other resources.
    - One protocol is to require each process to request and be allocated all its resources
      before it begins execution
    - Another is to allow request for resources only when process has none
      
    Both protocols have disadvantages however since allocated resources may be unused for
    a long time. Also, starvation of processes is possible since one of the many requested
    resources may be always used by some other process.
**** No preemption
     No premption of resources that have already been allocated. 
     Protocol: 
     If process is holding some resources and requests another resource that cannot be
     immediately allocated (i.e will have to wait), then all currently held resources are
     preempted. The pro cess will restart only when it can regain its old resources as well
     as receiving the new ones it is requesting.
     
**** Circular wait
     One way to ensure this condition never holds is to impose a total ordering of all
     resource types and require each process request resources in an increasing order
     of enumeration.
     
     _Proof by contradition circular wait cannot hold_
     Let the set of processes involved in the circular wait be {P0, P1, ..., Pn} where Pi
     is waiting for a resource Ri which is held by process Pi+1 (let Rn be held by P0). 
     Since process Pi+1 is holding resource Ri while requesting Ri+1, we must have 
     F(Ri) < F(Ri+1) for all i. But this would be mean F(Rn) < F(R0) which is impossible.

*** Deadlock avoidance
    Deadlock avoidance algorithm dynamically examines the resource-allocation state
    to ensure a circulate wait condition never exists. *state* is de fined by number of 
    available and allocated resources and maximum demands of the processes.

    - Safe State :: system can allocate resources to each process in some order and 
                    still avoid a deadlock. Formally, a system is in safe state iff there
                    exists a *safe sequence*. 
    - Safe sequence :: A sequence of processes <P1 , P2 , ..., Pn > 
                       is a safe sequence for the current allocation state if, 
                       for each Pi , the resource requests that Pi can make 
                       can be satisÔ¨Åed by the currently available resources 
                       plus the resources held by all Pj , with j < i.

    With this scheme, if a requested resource is available, it may still have to 
    wait thus overall resource utilization may be lower than it would be.

**** Resource Allocation Graph Algorithms
    - Before request is granted to process, claim edges are checked to make sure
      no cycle will occur once claim edges are converted to assignment edges. 
      O(n^2)

    - Bankers Algorithm :: Above algorithm not applicable to resource allocation
         system with multiple instances of each resource type. Four data structures
         required:
      - Available :: # avail resource of each  type
      - Max :: 2D matrix defined maximum demand of each process
      - Allocation :: 2D matrix defined number of resources of each type 
                      currently allocated
      - Need :: 2D matrix indicates remaining resource need of each process

         Each process must apriori claim its maximum use of processes. O(mn^2)

*** Deadlock Detection
    When should we invoke detection algorithm
    Must answer:
    1) How often is a deadlock likely to occur?
    2) How many processes will be affected by deadlock when it happens?

    Deadlock detection algorithms run in O(mn^2)

*** Recover from Deadlock
    Two options for breaking a deadlock
    1) Abort all deadlocked processes - can be expensive and costly
    2) Abort one process at a time until deadlock cycle eliminated - overhead

**** Resource premption
     We successuly prempt some resources from processes and give these resources
     to other processes until the deadlock cycle is broken. For premption is
     required for dealing with deadlocks, 3 issues must be addressed:
     1) Selecting a victim
     2) Rollback - how should we restart the prempted process in the future?
     3) Starvation - How do we guarantee resource will not always be preempted
        from the same process?

* Chapter 8 Main Memory
  Main memory are rigisters are the only storage the CPU has direct access to. Since that
  means there are no instructions that take disk addresses, data must be moved to memory
  before CPU can operate on them.

  Only kernal can set base and limit address of a process to ensure user process does
  not attempt to access memory outside what is given. Hardware support exists to trap
  to OS monitor if it detects unauthorized memory access

** Address Binding
   Programs on disk must be brought into memory for a process to execute them. Depending
   on the memory management, process may be moved between disk and memory during 
   execution. Processes on disk waiting to be brought into memory is called *input queue*

   A compiler will typically *bind* symbolic addresses of source programs to relocatable 
   address (e.g 14 bytes from beginning of this module). The linker then will in turn 
   bind the relocatable address to absolute address in memory. 
   
   The binding of instruction and data to memory addresses can be done at:
   - Compile time :: Know at compile time where process will reside in memory
   - Load time :: If not known at compile time, then compiler generates
                  *relocatable code* in regards to the user program's base address
   - Execution time :: If process can be moved during execution from one memory
                       segment to another. Most general OS use this method.
                       (context switch ?)
                       This type of binding makes the logical and physical address spaces
                       to differ.
   
** Logical vs. Physical Address Space
   - Logical Address :: generated by CPU
   - Physical Address :: seen by memory unit
   - Memory management unit :: device to allow execution-time mapping from logical 
        to physical address
   - Dynamic loading :: All routines are kept on disk in a relocatable formate until it is 
        called
   - Static linking :: system language libraries are compiled into one executable. With this,
                       every exectuable woule require a copy of the system library.
   - Dynamically linked libraries :: linking is postponed until execution time, thus
        executable programs do not need copies of the system library.

** Swapping
   Although a process must reside in memory, it can be temporarily placed in backing store 
   and then brought back into memory for continued execution.

** Contiguous Memory Allocation
   Main memory must accomodate both OS and user processes thus require an efficient way to 
   allocate main memory (e.g. contiguous memory allocation)

   int CMA, each process is contained in a single contiguous section of memory

*** Memory allocation
     In variable partition scheme, OS keeps a table indication which parts of memory
     are available and which are occupied. Available memory are referred to as a *hole*

     At any given time, we have list of available block sizes and an input queue. 
     THe OS can order the input queue according to scheduling alg. 

     - Dynamic storage allocation problem ::
          Concerns how to satisfy a request of size n from a list of free holes
          Strategies:
       - First fit :: allocate first hold big enough
       - Best fit :: allocate smallest hole big enough
       - Worst fit :: allocate largest hold

     Compaction - solution to external fragmentation.                      

     Another possible solution to external-fragmentation problem is to permit the logical
     address space of the process to be noncontiguous, thus paging and segmentation

** Paging
   Memory management scheme that permits physical address space of process to be
   contiguous.

   *FRAMES AND PAGES ARE THE SAME SIZE. PAGE TABLE PROVIDES A MAPPING BETWEEN THE TWO!*
   32-bit page entries can map to 2^32 frames. If each frame is 2^12, then total
   mapping of 2^(32 + 12) bytes are available!!!

*** Basic method
    Physical memory is broken down into fixed size chunks called *frames*, and logical memory
    is broken down into same size chunks called *pages*.

    When process is to executed, its pages are loaded into any available memory frames from 
    their source (e.g fs, backing store).

    Every logical address generated by CPU is divied into the *page number (p)* and a 
    *page offset (d)*. Page number is used to index into a page table. Page table contains the
    base address of each page in physical memory.

    Page size and frame size is defined by the hardware. Typically size is a power of 2, varying
    between 512 bytes to 16 MB per page.

    If size of logical address space is 2^m and page size 2^m, then the higher order m-n
    bits index into the page table, and the n lower bits designate page offset.

    As a concrete (although minuscule) example, consider the memory in
    Figure 8.9. Here, in the logical address, n= 2 and m = 4. Using a page size
    of 4 bytes and a physical memory of 32 bytes (8 pages), we show how the
    user‚Äôs view of memory can be mapped into physical memory. Logical address
    0 is page 0, offset 0. Indexing into the page table, we Ô¨Ånd that page 0 is in frame
    5. Thus, logical address 0 maps to physical address 20 [= (5 √ó 4) + 0]. Logical
    address 3 (page 0, offset 3) maps to physical address 23 [= (5 √ó 4) + 3]. Logical
    address 4 is page 1, offset 0; according to the page table, page 1 is mapped to
    frame 6. Thus, logical address 4 maps to physical address 24 [= (6 √ó 4) + 0].
    Logical address 13 maps to physical address 9.

    
    | [[file:F8_9.png]] |
    
    A 32-bit entry can point to one of 2^32 physical page frames. If each frame size is
    4 KB (2^12) then a system with 4 byte entries can address 2^44 (16 TB) of physical memory

    *page-table base register (PTBR)* 
    Page tables are kept in memory for large page tables cannot be kept in memory. This means
    Two memory access must be made to access one bye (e.g first access for finding the page
    table entry in memory, second access for finding the byte in memory with the retrieved
    page table entry

    *translation look-aside buffer (TLB)*
    Hardware cache to speed up page table lookup. Expensive however thus only 64 - 1024 entries.
    TLB contains only a few of the page-table entries. When logical address is generated
    by the CPU, it page number is presented to TLB. If found, the frame number value is given.
    else we get what is called *TLB miss*

** Structure of Page Table 
   Since most modern computer support large logical address space, page table can
   become excessively large. For example, if we have 32 bit logical address space,
   and the page size or frame in the system is 4 KB (2^12), we would require 
   (2^32)/(2^12) = 2^20 entries for the page table. If each page table entry is 4 bytes,
   that requires 4*2^20 = 4 MB physical address space for the page table alone.

   Solutions:
   - Hierarchical Page Table - two-level paging algorithm :: 
        Page table itself is also paged. This allows us to keep smaller chunks of 
        page tables in memory. Don't have to load one huge page table.
        
   - Hashed Page Tables :: 
        common approach to handling address space larger than 32 bits. Hash values 
        are the virtual page number.

   - Inverted Page Tables ::
        To solve the problem large amounts of memory allocated to page tables.
        One entry for each real page (or frame) of memory.
        Entry consits of the virtual address of the page stored 
 
** Segmentation
   Memory management scheme that supports the view of memory where one does not convern
   the location of methods, procedures, functions, data structures, variables, libraries
   etc in memory they are occupied.

   Logical address space is a collection of segments. Each segment has a name and length.
   The addresses specify both the segment name and offsets within the segment. The user
   therefore specifies each address by the segment name and offset.

   Contrast this scheme with the paging scheme in which user specifies a single address,
   which is partitioned by hardware into page number and an offset, transparent to the
   programmer.

   When a program is compiled, a C compiler may create separate segments for the following:
   1) Code
   2) Global variables
   3) Heap
   4) Stack
   5) C library

   Between the segmented logical address to the linear address for finding physical address,
   these exists a segment table.

   [[file:F8_20.png]]      

* Chapter 9 Virtual Memory
  Technique that allows the execution of processes that are not completely in memory,
  thus allowing programs larger than memory

  *Virtual memory*
  Involes separation of logical memory as perceived by users from physical memory
  to allow perceived memory to be larger than there is available

** Demand Paging
   A technique of loading only what is needed rather than loading the whole executable
   program. *pager* is what brings only the required pages into memory.

   A way to distinguish pages that are in memory and pages are in disk is to use a 
   bit scheme in the page table. 

   When process attempts to access page that is not marked as valid, a *page fault* will occur.
   During this transition of control, the required page is brought into memory from disk
   and the page table is updated. Control is returned to the process as though nothing 
   had happened.

   *pure demand paging* is when process continues to page fault whenever the requested page
   is not in memory until every page that it needs is in memory.

   Hardware to support demand paging (same as for paging and swapping):
   - Page Table :: to mark an entry as valid or invalid
   - Secondary memory :: to hold pages that are not present in memory.

   *copy on write* - Upon forking a child process, rather than creating a copy of the
   parent's address space, the pages are shared until a write is required, which then the
   shared page will be created. Useful in cases where exec() is called right after fork(),
   since we can avoid unecessary copying.

** Page replacement
   Solution to when there are no free frames left to bring the page residing on disk.
   IF no frame is free, we find one not being currently used and free it by writing
   it out to swap space.. We change page table to indicate page is no longer in memory.

   Updated page fault service routine:
   1) Find location of desired page on disk
   2) Find a free frame:
      a) If there exists free frame, use it
      b) If no free frame, use page replacement to find victim frame
      c) Write victim frame to disk; change the page and page tables accordingly
   3) Read desired page into newly freed frame
   4) Restart the user process

   Note if no frames are free, TWO page transfers (out, int) are required. 
   - Can reduce this overhead via *modify bit* to indicate page had been modified.



